{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who's that Pokémon : Transfer Learning For Pokémon Type Image Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Nathan Yao <br>\n",
    "Github: https://github.com/nathanyao13"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________________________\n",
    "![Gen 1 Starters](https://imgur.com/uLKRVW7.png)\n",
    "________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a technique that leverages a model previously trained on a large dataset and adapts it to a new, related task. It is especially popular in \n",
    "image classification, where training a model from scratch would typically require vast amounts of data and computational resources—resources that may not be accessible to individuals like myself.<br>\n",
    "<br>\n",
    "In this project, I applied transfer learning using both ResNet18 and ResNet50 architectures to classify Pokémon images. The appeal of ResNet models lies in  their use of Residual Blocks, which are designed to address the challenges that come with training very deep neural networks.<br>\n",
    "<br>\n",
    "Rather than learning a direct mapping between the input and output of each layer, residual blocks focus on learning the residual—the change that needs to be applied to the input. This is achieved by adding shortcut (or skip) connections that pass the input directly to deeper layers, allowing gradients to flow more easily during backpropagation. This structure helps mitigate the problem of performance degradation that occurs when network depth increases, even in the absence of overfitting.<br>\n",
    "<br>\n",
    "These residual connections enable the training of very deep networks that outperform their shallower counterparts.<br>\n",
    "<br>\n",
    "\n",
    "![resblock](https://imgur.com/hBbqqC8.png)\n",
    "\n",
    "### Tasks and Models<br>\n",
    "\n",
    "• Image Classification with 18 type classes (All Pokémon Types) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;→ ResNet18 and ResNet50<br>\n",
    "\n",
    "• Image Classification with 3 type classes (Starter Types: Fire, Water, Grass) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;→ ResNet18 and ResNet50<br>\n",
    "<br>\n",
    "\n",
    "This notebook contains the implementation of the **ResNet18 transfer learning** <br>\n",
    "model, specifically for classifying Pokémon as one of the three starter types: <br>\n",
    "**Fire**, **Water**, or **Grass**.<br>\n",
    "________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from dataset_class import PokemonImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_name</th>\n",
       "      <th>primary_type</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bulbasaur</td>\n",
       "      <td>grass</td>\n",
       "      <td>pokemon_images/Bulbasaur.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ivysaur</td>\n",
       "      <td>grass</td>\n",
       "      <td>pokemon_images/Ivysaur.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>venusaur</td>\n",
       "      <td>grass</td>\n",
       "      <td>pokemon_images/Venusaur.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>charmander</td>\n",
       "      <td>fire</td>\n",
       "      <td>pokemon_images/Charmander.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>charmeleon</td>\n",
       "      <td>fire</td>\n",
       "      <td>pokemon_images/Charmeleon.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>charizard</td>\n",
       "      <td>fire</td>\n",
       "      <td>pokemon_images/Charizard.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>squirtle</td>\n",
       "      <td>water</td>\n",
       "      <td>pokemon_images/Squirtle.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wartortle</td>\n",
       "      <td>water</td>\n",
       "      <td>pokemon_images/Wartortle.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>blastoise</td>\n",
       "      <td>water</td>\n",
       "      <td>pokemon_images/Blastoise.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english_name primary_type                     image_path\n",
       "0    bulbasaur        grass   pokemon_images/Bulbasaur.jpg\n",
       "1      ivysaur        grass     pokemon_images/Ivysaur.jpg\n",
       "2     venusaur        grass    pokemon_images/Venusaur.jpg\n",
       "3   charmander         fire  pokemon_images/Charmander.jpg\n",
       "4   charmeleon         fire  pokemon_images/Charmeleon.jpg\n",
       "5    charizard         fire   pokemon_images/Charizard.jpg\n",
       "6     squirtle        water    pokemon_images/Squirtle.jpg\n",
       "7    wartortle        water   pokemon_images/Wartortle.jpg\n",
       "8    blastoise        water   pokemon_images/Blastoise.jpg"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pokemon.csv')\n",
    "df[['english_name','primary_type','image_path']].head(9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying a Pokémon's Primary type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['image_path', 'primary_type']) \n",
    "df['main_type'] = df['primary_type'].str.strip()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (df[\"main_type\"] == \"grass\") | (df[\"main_type\"] == \"fire\") | (df[\"main_type\"] == \"water\")\n",
    "df = df[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['grass', 'fire', 'water']\n"
     ]
    }
   ],
   "source": [
    "type_names = df['main_type'].unique().tolist()\n",
    "print(\"Classes:\", type_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (Encoded): [1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['encoded_type'] = label_encoder.fit_transform(df['main_type'])\n",
    "type_encoded = df['encoded_type'].unique().tolist()\n",
    "print(\"Classes (Encoded):\", type_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes:  3\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(type_encoded)\n",
    "print(\"Number of Classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, \n",
    "                                    stratify=df['encoded_type'], \n",
    "                                    random_state = 13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PokemonImages(df = train_df, label = \"encoded_type\", transform=trans)\n",
    "val_dataset = PokemonImages(df = val_df, label = \"encoded_type\", transform=trans)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18 Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Resnet archiecture](https://imgur.com/0Zdb0Oq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = models.resnet18(weights= models.ResNet18_Weights.DEFAULT)\n",
    "resnet18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the CNN "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the architecture above, we observe that the final layer of the model is a fully connected linear layer.  \n",
    "This is the layer we want to modify and train for our specific classification task.  \n",
    "\n",
    "However, the original output dimension of this layer is 1000, which corresponds to the ImageNet dataset.  \n",
    "Since our task only has 3 classes, we need to adjust this final layer.\n",
    "\n",
    "To do this, we:\n",
    "\n",
    "1. Freeze all layers in the pretrained model so that their weights remain unchanged.\n",
    "2. Replace the final linear layer with a new one that outputs the desired number of classes.\n",
    "\n",
    "This approach allows us to take advantage of the rich feature representations learned by the pretrained ResNet model,  \n",
    "while only training a small number of parameters in the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezes the whole model\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Reseting the Final Layer and param.requires_grad = True by default \n",
    "# Final Layer is resnet18.fc \n",
    "final_layer_features = resnet18.fc.in_features\n",
    "\n",
    "# Outputting new number of Classes\n",
    "resnet18.fc = nn.Linear(final_layer_features, num_classes)\n",
    "\n",
    "# Cross Entropy Loss for Multiple Classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizing all Parameters in the final linear layer using SGD\n",
    "optimizer = optim.SGD(resnet18.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decrease Learning Rate by a factor of 0.1 every 7 epochs\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # training mode\n",
    "                else:\n",
    "                    model.eval()   # eval mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # iterating through data\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients every iteration\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # SGD for backwards\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights based on validation results\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.0937 Acc: 0.4366\n",
      "val Loss: 1.0061 Acc: 0.5370\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.9513 Acc: 0.5493\n",
      "val Loss: 0.8766 Acc: 0.6111\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.8320 Acc: 0.6667\n",
      "val Loss: 0.7721 Acc: 0.6296\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.7085 Acc: 0.7136\n",
      "val Loss: 0.6839 Acc: 0.6852\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.6386 Acc: 0.7981\n",
      "val Loss: 0.6205 Acc: 0.7593\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.5736 Acc: 0.7934\n",
      "val Loss: 0.5970 Acc: 0.7593\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.5530 Acc: 0.7840\n",
      "val Loss: 0.5671 Acc: 0.7778\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.5096 Acc: 0.8075\n",
      "val Loss: 0.5556 Acc: 0.7963\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "final_model = train_model(resnet18, criterion, optimizer,\n",
    "                         lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "## Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_samples(model, num_images=6):\n",
    "    model.eval()\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    "    )\n",
    "\n",
    "    indices = random.sample(range(len(val_dataset)), num_images)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = val_dataset[idx]\n",
    "        label = int(label)\n",
    "\n",
    "        input_tensor = img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            pred = int(torch.argmax(output, dim=1).item())\n",
    "\n",
    "        img_disp = inv_normalize(img).permute(1, 2, 0).numpy()\n",
    "        img_disp = np.clip(img_disp, 0, 1)\n",
    "\n",
    "        true_type = label_encoder.inverse_transform([label])[0]\n",
    "        pred_type = label_encoder.inverse_transform([pred])[0]\n",
    "\n",
    "        ax = plt.subplot((num_images + 1) // 2, 2, i + 1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Pred: {pred_type}, True: {true_type}')\n",
    "        ax.imshow(img_disp)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Compose' object has no attribute 'Normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvisualize_random_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mvisualize_random_samples\u001b[39m\u001b[34m(model, num_images)\u001b[39m\n\u001b[32m      2\u001b[39m model.eval()\n\u001b[32m      3\u001b[39m fig = plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m10\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m inv_normalize = \u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNormalize\u001b[49m(\n\u001b[32m      6\u001b[39m     mean=[-\u001b[32m0.485\u001b[39m / \u001b[32m0.229\u001b[39m, -\u001b[32m0.456\u001b[39m / \u001b[32m0.224\u001b[39m, -\u001b[32m0.406\u001b[39m / \u001b[32m0.225\u001b[39m],\n\u001b[32m      7\u001b[39m     std=[\u001b[32m1\u001b[39m / \u001b[32m0.229\u001b[39m, \u001b[32m1\u001b[39m / \u001b[32m0.224\u001b[39m, \u001b[32m1\u001b[39m / \u001b[32m0.225\u001b[39m]\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m indices = random.sample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val_dataset)), num_images)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indices):\n",
      "\u001b[31mAttributeError\u001b[39m: 'Compose' object has no attribute 'Normalize'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_random_samples(final_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________________________\n",
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 Type Classification**:\n",
    "- ResNet18: \n",
    "    - Time to Train: 3m 20s\n",
    "    - Best Validation Accuracy: 83.33%\n",
    "- ResNet50:  \n",
    "    - Time to Train: 11m 56s\n",
    "    - Best Validation Accuracy: 66.66%\n",
    "\n",
    "**18 Type Classification**:\n",
    "- ResNet18:\n",
    "    - Time to Train: 12m 45s\n",
    "    - Best Validation Accuracy: 35.00%\n",
    "- ResNet50: \n",
    "    - Time to Train:39m 16s\n",
    "    - Best Validation Accuracy: 28.88% \n",
    "\n",
    "(Note: These results all have standard deviations due to randomness) <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ResNet50 runs had lower accuracy compared to **ResNet18**. One major factor contributing to this is the limited size of the dataset.  \n",
    "ResNet50, being a significantly deeper network, is more prone to overfitting when trained on small datasets. This results in poor generalization and lower validation accuracy, despite its theoretical capacity for better performance from Pytorch's documentation.\n",
    "\n",
    "Furthermore, classifying Pokémon into 18 types is inherently a more challenging task than classifying into just 3 types.  \n",
    "The 18-type classification problem involves:\n",
    "- More class imbalance (some types are far more common than others)\n",
    "- Finer visual distinctions between classes\n",
    "- Increased output dimensionality, requiring the model to make more nuanced decisions\n",
    "\n",
    "This added complexity leads to:\n",
    "- Longer training times due to a more difficult optimization landscape\n",
    "- Lower validation accuracy, since the model has a harder time capturing discriminative features across so many categories\n",
    "\n",
    "In contrast, the 3-type classification task (Fire, Water, Grass) is more balanced and visually distinguishable, enabling ResNet18 to achieve over 83% accuracy with relatively short training time.\n",
    "\n",
    "Additionally, the performance gap between ResNet18 and ResNet50 highlights that model depth isn't always better—especially when:\n",
    "- The dataset is small\n",
    "- The task complexity doesn’t justify deeper architectures\n",
    "- Computational cost and overfitting become concerns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Primary and Secondary Typing**  \n",
    "It is well known that Pokémon may have one or two types. The first is considered their primary type, and the second (if present) is their secondary type.  \n",
    "While this project focused only on primary types, including secondary types introduces a more complex image classification task.  \n",
    "Some model errors appear to stem from confusion involving dual-typed Pokémon.  \n",
    "In future work, a multi-label classification approach could be implemented to address this.\n",
    "\n",
    "2. **Accuracy and Dataset Limitations**  \n",
    "The accuracy of the models is inherently limited by the size of the dataset—currently only ~800 Pokémon.  \n",
    "With each new generation, more Pokémon are introduced, expanding the dataset.  \n",
    "As the franchise grows, so will the potential for building better-performing models.  \n",
    "If someone revisits this task in the future with an expanded dataset, they may achieve significantly better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
